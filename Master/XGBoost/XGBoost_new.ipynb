{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import system\n",
    "from numpy import inf\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import pickle\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO, PSOAnimation\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.factory import get_termination\n",
    "from pymoo.core.callback import Callback\n",
    "\n",
    "from pymoo.core.mixed import MixedVariableGA\n",
    "from pymoo.core.variable import Real, Integer\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "from pymoo.util.display.column import Column\n",
    "from pymoo.util.display.output import Output\n",
    "\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.core.variable import Real, Integer, Choice, Binary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../preprocessing/processed_data/wine.csv')\n",
    "y_wine = df[['quality']]\n",
    "x_wine = df.drop(columns='quality')\n",
    "x_wine = x_wine.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 32\n",
    "POPULATION = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFOLD_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_min = 3\n",
    "n_estimators_max = 1000\n",
    "learning_rate_min = 0.001\n",
    "learning_rate_max = 0.3\n",
    "max_depth_min = 3\n",
    "max_depth_max = 15\n",
    "subsample_min = 0.05\n",
    "subsample_max = 1.0\n",
    "colsample_bytree_min = 0.2\n",
    "colsample_bytree_max = 1.0\n",
    "gamma_min = 0\n",
    "gamma_max = 10\n",
    "min_child_weight_min = 0\n",
    "min_child_weight_max = 10\n",
    "reg_lambda_min = 0\n",
    "reg_lambda_max = 1\n",
    "reg_alpha_min = 0\n",
    "reg_alpha_max = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOutput(Output):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        global pbar \n",
    "        pbar = tqdm(total=ITERATIONS)\n",
    "        self.score = Column(\"score\", width=13)\n",
    "        self.Parameters = Column(\"Parameters\", width=35)\n",
    "        self.columns += [self.score, self.Parameters]\n",
    "\n",
    "    def update(self, algorithm):\n",
    "        super().update(algorithm)\n",
    "        self.score.set(-np.min(algorithm.pop.get(\"F\")))\n",
    "        #self.Parameters.set(algorithm.pop.get(\"X\")[0])\n",
    "        pbar.update(1)\n",
    "        if pbar.n == ITERATIONS: pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedVariableProblem(ElementwiseProblem):\n",
    "    def __init__(self, **kwargs):\n",
    "        #params reference 1\n",
    "        xgb_params = {\n",
    "        'n_estimators' : Integer(bounds=(n_estimators_min, n_estimators_max)), #high num\n",
    "        'learning_rate' : Real(bounds=(learning_rate_min, learning_rate_max)),\n",
    "        'max_depth' : Integer(bounds=(max_depth_min, max_depth_max)),\n",
    "        'subsample' : Real(bounds=(subsample_min, subsample_max)),\n",
    "        'colsample_bytree' : Real(bounds=(colsample_bytree_min, colsample_bytree_max)),\n",
    "        'gamma'            : Real(bounds=(gamma_min, gamma_max)),\n",
    "        'min_child_weight' : Real(bounds=(min_child_weight_min, min_child_weight_max)),\n",
    "        'reg_lambda'       : Real(bounds=(reg_lambda_min, reg_lambda_max)), #l2 lambda\n",
    "        'reg_alpha'        : Real(bounds=(reg_alpha_min, reg_alpha_max)), #L1 alpha\n",
    "        }\n",
    "        super().__init__(vars=xgb_params, n_obj=1, **kwargs)\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        n_estimators = X['n_estimators']\n",
    "        learning_rate = X['learning_rate']\n",
    "        max_depth = X['max_depth']\n",
    "        subsample = X['subsample']\n",
    "        colsample_bytree = X['colsample_bytree']\n",
    "        gamma = X['gamma']\n",
    "        min_child_weight = X['min_child_weight']\n",
    "        reg_lambda = X['reg_lambda']\n",
    "        reg_alpha = X['reg_alpha']\n",
    "        \n",
    "        model_xgboost = xgb.XGBClassifier(\n",
    "            n_estimators = n_estimators,\n",
    "            learning_rate = learning_rate,\n",
    "            max_depth = max_depth,\n",
    "            subsample = subsample,\n",
    "            colsample_bytree = colsample_bytree,\n",
    "            gamma = gamma,\n",
    "            min_child_weight = min_child_weight,\n",
    "            reg_lambda = reg_lambda,\n",
    "            reg_alpha = reg_alpha,\n",
    "            n_jobs = -1\n",
    "            )\n",
    "        \n",
    "        kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "        \n",
    "        scores = cross_val_score(model_xgboost,  x_wine, y_wine, cv = kfold, n_jobs=-1, scoring='f1_weighted')  \n",
    "        result = scores.mean()     \n",
    "        if result == np.nan:\n",
    "            result = 0\n",
    "        #print(result)\n",
    "        #return result\n",
    "        out[\"F\"] = -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f1_ga(ITERATIONS=32, POPULATION=32):\n",
    "    problem = MixedVariableProblem()\n",
    "\n",
    "    algorithm = MixedVariableGA(pop_size=POPULATION)\n",
    "\n",
    "    term = get_termination(\"n_gen\", ITERATIONS)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    res = minimize(problem,\n",
    "                   algorithm,\n",
    "                   termination=term,\n",
    "                   verbose=True,\n",
    "                   output=MyOutput(),\n",
    "                   # seed=1,\n",
    "                   )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    #print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "    #print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))\n",
    "    params, score = res.X, res.F[0]\n",
    "    return params, -score, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_ga(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_f1_ga()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['GA'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pymoo.org/algorithms/soo/pso.html?highlight=pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_range(x, max_val, min_val):\n",
    "    scaled_value = min_val + (max_val - min_val) * (x / 1000)\n",
    "    \n",
    "    return scaled_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_integers(positions, lower_bound, upper_bound):\n",
    "    return np.clip(np.round(positions), lower_bound, upper_bound).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyswarms as ps\n",
    "from pyswarms.utils.functions import single_obj as fx\n",
    "import numpy as np\n",
    "\n",
    "n_estimators_min_temp, max_depth_min_temp = 0, 0\n",
    "xl=np.array([\n",
    "            n_estimators_min_temp, #n_estimators_min,\n",
    "            learning_rate_min,\n",
    "            max_depth_min_temp, #max_depth_min,\n",
    "            subsample_min,\n",
    "            colsample_bytree_min,\n",
    "            gamma_min,\n",
    "            min_child_weight_min,\n",
    "            reg_lambda_min,\n",
    "            reg_alpha_min,\n",
    "            ])\n",
    "n_estimators_max_temp, max_depth_max_temp = 1, 1\n",
    "xu=np.array([\n",
    "            n_estimators_max_temp, #n_estimators_max,\n",
    "            learning_rate_max,\n",
    "            max_depth_max_temp, #max_depth_max,\n",
    "            subsample_max,\n",
    "            colsample_bytree_max,\n",
    "            gamma_max,\n",
    "            min_child_weight_max,\n",
    "            reg_lambda_max,\n",
    "            reg_alpha_max,\n",
    "            ])\n",
    "\n",
    "def PSO_Optimize_F1(values):\n",
    "    x = values[0] \n",
    "    model_xgboost = xgb.XGBClassifier(\n",
    "        n_estimators = round_to_integers(x[0] * n_estimators_max, n_estimators_min, n_estimators_max),\n",
    "        learning_rate = x[1],\n",
    "        max_depth = round_to_integers(x[2] * max_depth_max, max_depth_min, max_depth_max),\n",
    "        subsample = x[3],\n",
    "        colsample_bytree = x[4],\n",
    "        gamma = x[5],\n",
    "        min_child_weight = x[6],\n",
    "        reg_lambda = x[7],\n",
    "        reg_alpha = x[8],\n",
    "        n_jobs = -1\n",
    "        )\n",
    "    \n",
    "    kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "    \n",
    "    scores = cross_val_score(model_xgboost,  x_wine, y_wine, cv = kfold, n_jobs=-1, scoring='f1_weighted')  \n",
    "    result = scores.mean()     \n",
    "\n",
    "    return -result\n",
    "\n",
    "def run_f1_pso():\n",
    "    # Call an instance of PSO\n",
    "    swarm_size = POPULATION\n",
    "    iters = ITERATIONS\n",
    "    dim = 9\n",
    "    options = {'c1': 1.5, 'c2':1.5, 'w':0.5}\n",
    "    constraints = (xl,xu)\n",
    "\n",
    "    optimizer = ps.single.GlobalBestPSO(n_particles=swarm_size,\n",
    "                                        dimensions=dim,\n",
    "                                        options=options,\n",
    "                                        ftol = -inf,\n",
    "                                        bounds=constraints)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    cost, joint_vars = optimizer.optimize(objective_func = PSO_Optimize_F1, iters=iters)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time    \n",
    "\n",
    "    params = []\n",
    "    for idx in range(len(joint_vars)):\n",
    "        if idx == 0:\n",
    "            params.append(round_to_integers(joint_vars[idx] * n_estimators_max, n_estimators_min, n_estimators_max))\n",
    "        elif idx == 2:\n",
    "            params.append(round_to_integers(joint_vars[idx] * max_depth_max, max_depth_min, max_depth_max))\n",
    "        else:\n",
    "            params.append(joint_vars[idx])\n",
    "\n",
    "    params_dict = {\n",
    "        'n_estimators' : params[0],\n",
    "        'learning_rate' : params[1],\n",
    "        'max_depth' : params[2],\n",
    "        'subsample' : params[3],\n",
    "        'colsample_bytree' : params[4],\n",
    "        'gamma' : params[5],\n",
    "        'min_child_weight' : params[6],\n",
    "        'reg_lambda' : params[7],\n",
    "        'reg_alpha' : params[8],\n",
    "                }\n",
    "    return params_dict, -cost, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_pso(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_f1_pso()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['PSO'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def random_search_int_range(min_l, max_l):\n",
    "    return (np.arange(randint.ppf(0.01, min_l, max_l),randint.ppf(0.99, min_l, max_l))).astype(int)\n",
    "\n",
    "\n",
    "def run_random_search_f1(iterations = 1024):\n",
    "    param_distribution = {\n",
    "    'n_estimators' : round_to_integers(uniform.rvs(0, 1, 1000) * n_estimators_max, n_estimators_min, n_estimators_max),\n",
    "    'learning_rate' : uniform(learning_rate_min, learning_rate_max),\n",
    "    'max_depth' : round_to_integers(uniform.rvs(0, 1, 1000) * max_depth_max, max_depth_min, max_depth_max),\n",
    "    'subsample' : uniform(subsample_min, subsample_max),\n",
    "    'colsample_bytree' : uniform(colsample_bytree_min, colsample_bytree_max),\n",
    "    'gamma' : uniform(gamma_min, gamma_max),\n",
    "    'min_child_weight' : random_search_int_range(min_child_weight_min, min_child_weight_max),\n",
    "    'reg_alpha' : uniform(reg_alpha_min, reg_alpha_max),\n",
    "    'reg_lambda' : uniform(reg_lambda_min, reg_lambda_max),\n",
    "    #Not used\n",
    "    #'scale_pos_weight' : random_search_int_range(scale_pos_weight_min, scale_pos_weight_max),\n",
    "    #'base_score' : uniform(base_score_min, base_score_max),\n",
    "    'n_jobs' :  [-1]\n",
    "    }\n",
    "    \n",
    "    kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    random_search_f1 = RandomizedSearchCV(xgb.XGBClassifier(), param_distribution, n_iter = iterations, n_jobs = -1, cv=kfold, verbose=False, scoring='f1_weighted')\n",
    "    random_search_f1.fit(x_wine, y_wine)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    params = random_search_f1.best_params_\n",
    "    score = random_search_f1.best_score_\n",
    "    \n",
    "    \n",
    "    return params, score, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_rs(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_random_search_f1()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['RS'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective_f1(trial):\n",
    "  \n",
    "      n_estimators = trial.suggest_int('n_estimators', n_estimators_min, n_estimators_max)\n",
    "      learning_rate = trial.suggest_float('learning_rate', learning_rate_min, learning_rate_max)\n",
    "      max_depth = trial.suggest_int('max_depth', max_depth_min, max_depth_max)\n",
    "      subsample = trial.suggest_float('subsample', subsample_min, subsample_max)\n",
    "      colsample_bytree = trial.suggest_float('colsample_bytree', colsample_bytree_min, colsample_bytree_max)\n",
    "      gamma = trial.suggest_float('gamma', gamma_min, gamma_max)\n",
    "      min_child_weight = trial.suggest_float('min_child_weight', min_child_weight_min, min_child_weight_max)\n",
    "      reg_alpha = trial.suggest_float('reg_alpha', reg_alpha_min, reg_alpha_max)\n",
    "      reg_lambda = trial.suggest_float('reg_lambda', reg_lambda_min, reg_lambda_max)\n",
    "      \n",
    "      model_xgboost = xgb.XGBClassifier(\n",
    "            n_estimators = n_estimators,\n",
    "            learning_rate = learning_rate,\n",
    "            max_depth = max_depth,\n",
    "            subsample = subsample,\n",
    "            colsample_bytree = colsample_bytree,\n",
    "            gamma = gamma,\n",
    "            min_child_weight = min_child_weight,\n",
    "            reg_lambda = reg_lambda,\n",
    "            reg_alpha = reg_alpha,\n",
    "            n_jobs = -1\n",
    "            )\n",
    "\n",
    "      \n",
    "      kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "      \n",
    "      result = cross_val_score(model_xgboost, x_wine, y_wine, n_jobs=-1, scoring='f1_weighted', cv=kfold).mean()\n",
    "      if result == np.nan:\n",
    "            result = 0\n",
    "\n",
    "      return result\n",
    "\n",
    "def run_optuna_f1(n_trials=1024):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study.optimize(objective_f1, n_trials=n_trials, n_jobs = -1)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time    \n",
    "    trial = study.best_trial\n",
    "    #print('F1: {}'.format(trial.value))\n",
    "    #print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "    return trial.params, trial.value, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_optuna(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_optuna_f1()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['Optuna'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import time\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "def objective_f1(config, x_train, y_train, cv):\n",
    "    n_estimators = int(config[\"n_estimators\"])\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    max_depth = int(config[\"max_depth\"])\n",
    "    subsample = config[\"subsample\"]\n",
    "    colsample_bytree = config[\"colsample_bytree\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    min_child_weight = config[\"min_child_weight\"]\n",
    "    reg_alpha = config[\"reg_alpha\"]\n",
    "    reg_lambda = config[\"reg_lambda\"]\n",
    "\n",
    "    model_xgboost = xgb.XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        gamma=gamma,\n",
    "        min_child_weight=min_child_weight,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    kfold = KFold(n_splits=cv, shuffle=True)\n",
    "    \n",
    "    result = cross_val_score(model_xgboost, x_train, y_train, n_jobs=-1, scoring='f1_weighted', cv=kfold).mean()\n",
    "    if np.isnan(result):\n",
    "        result = 0\n",
    "\n",
    "    return result\n",
    "\n",
    "def run_flaml_f1(x_train, y_train, n_trials=1024, cv=3):\n",
    "    automl = AutoML()\n",
    "    \n",
    "    settings = {\n",
    "        \"time_budget\": 600,  # set your desired time budget\n",
    "        \"metric\": 'f1',\n",
    "        \"task\": 'classification',\n",
    "        \"log_file_name\": 'flaml.log',\n",
    "    }\n",
    "    \n",
    "    automl.fit(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        task='classification',\n",
    "        estimator_list=['xgboost'],\n",
    "        eval_method='f1',\n",
    "        cv=cv,\n",
    "        **settings\n",
    "    )\n",
    "\n",
    "    best_trial = automl.best_config\n",
    "    best_value = automl.best_perf\n",
    "    execution_time = automl.time_elapsed\n",
    "\n",
    "    return best_trial, best_value, execution_time\n",
    "\n",
    "# Example usage\n",
    "# best_trial, best_value, execution_time = run_flaml_f1(x_train, y_train, n_trials=1024, cv=5)\n",
    "# print(\"Best hyperparameters:\", best_trial)\n",
    "# print(\"Best f1 score:\", best_value)\n",
    "# print(\"Execution time:\", execution_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 01-18 20:04:56] {1679} INFO - task = classification\n",
      "[flaml.automl.logger: 01-18 20:04:56] {1690} INFO - Evaluation method: cv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 20:04:56,415 - flaml.automl.task.generic_task - INFO - class 0 augmented from 6 to 24\n",
      "2024-01-18 20:04:56,415 - flaml.automl.task.generic_task - INFO - class 5 augmented from 16 to 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 01-18 20:04:56] {1788} INFO - Minimizing error metric: f1_weighted\n",
      "[flaml.automl.logger: 01-18 20:04:56] {1900} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
      "[flaml.automl.logger: 01-18 20:04:56] {2218} INFO - iteration 0, current learner xgboost\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'search_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 33\u001b[0m\n\u001b[0;32m     21\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m :  \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv\u001b[39m\u001b[38;5;124m\"\u001b[39m: cv,\n\u001b[0;32m     29\u001b[0m }\n\u001b[0;32m     31\u001b[0m automl \u001b[38;5;241m=\u001b[39m AutoML()\n\u001b[1;32m---> 33\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_wine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_wine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m best_config \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mbest_config\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\automl.py:1928\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[1;34m(self, X_train, y_train, dataframe, label, metric, task, n_jobs, log_file_name, estimator_list, time_budget, max_iter, sample, ensemble, eval_method, log_type, model_history, split_ratio, n_splits, log_training_metric, mem_thres, pred_time_limit, train_time_limit, X_val, y_val, sample_weight_val, groups_val, groups, verbose, retrain_full, split_type, learner_selector, hpo_method, starting_points, seed, n_concurrent_trials, keep_search_state, preserve_checkpoint, early_stop, force_cancel, append_log, auto_augment, min_sample_size, use_ray, use_spark, free_mem_ratio, metric_constraints, custom_hp, time_col, cv_score_agg_func, skip_transform, mlflow_logging, fit_kwargs_by_estimator, **fit_kwargs)\u001b[0m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1928\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_estimator:\n\u001b[0;32m   1930\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\automl.py:2482\u001b[0m, in \u001b[0;36mAutoML._search\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2480\u001b[0m     state\u001b[38;5;241m.\u001b[39mbest_config \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39minit_config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39minit_config \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_ray \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m-> 2482\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_parallel()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\automl.py:2318\u001b[0m, in \u001b[0;36mAutoML._search_sequential\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2312\u001b[0m         search_state\u001b[38;5;241m.\u001b[39msearch_alg\u001b[38;5;241m.\u001b[39msearcher\u001b[38;5;241m.\u001b[39mset_search_properties(\n\u001b[0;32m   2313\u001b[0m             metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2314\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2315\u001b[0m             metric_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mbest_loss,\n\u001b[0;32m   2316\u001b[0m         )\n\u001b[0;32m   2317\u001b[0m start_run_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 2318\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_alg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_alg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_budget_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_budget_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2323\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_spark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2326\u001b[0m time_used \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_run_time\n\u001b[0;32m   2327\u001b[0m better \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\tune\\tune.py:808\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(evaluation_function, config, low_cost_partial_config, cat_hp_cost, metric, mode, time_budget_s, points_to_evaluate, evaluated_rewards, resource_attr, min_resource, max_resource, reduction_factor, scheduler, search_alg, verbose, local_dir, num_samples, resources_per_trial, config_constraints, metric_constraints, max_failure, use_ray, use_spark, use_incumbent_result_in_evaluation, log_file_name, lexico_objectives, force_cancel, n_concurrent_trials, **ray_args)\u001b[0m\n\u001b[0;32m    806\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PySparkOvertimeMonitor(time_start, time_budget_s, force_cancel):\n\u001b[1;32m--> 808\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_to_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\state.py:302\u001b[0m, in \u001b[0;36mAutoMLState._compute_with_config_base\u001b[1;34m(config_w_resource, state, estimator, is_report)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLAML_sample_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    288\u001b[0m budget \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mtime_budget \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (state\u001b[38;5;241m.\u001b[39mtime_budget \u001b[38;5;241m-\u001b[39m state\u001b[38;5;241m.\u001b[39mtime_from_start) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sample_size \u001b[38;5;241m/\u001b[39m state\u001b[38;5;241m.\u001b[39mdata_size[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    296\u001b[0m (\n\u001b[0;32m    297\u001b[0m     trained_estimator,\n\u001b[0;32m    298\u001b[0m     val_loss,\n\u001b[0;32m    299\u001b[0m     metric_for_logging,\n\u001b[0;32m    300\u001b[0m     _,\n\u001b[0;32m    301\u001b[0m     pred_time,\n\u001b[1;32m--> 302\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_estimator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampled_X_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampled_y_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_time_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_time_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearner_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_score_agg_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_training_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_estimator_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mretrain_final \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state\u001b[38;5;241m.\u001b[39mmodel_history:\n\u001b[0;32m    325\u001b[0m     trained_estimator\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\ml.py:369\u001b[0m, in \u001b[0;36mcompute_estimator\u001b[1;34m(X_train, y_train, X_val, y_val, weight_val, groups_val, budget, kf, config_dic, task, estimator_name, eval_method, eval_metric, best_val_loss, n_jobs, estimator_class, cv_score_agg_func, log_training_metric, fit_kwargs, free_mem_ratio)\u001b[0m\n\u001b[0;32m    351\u001b[0m     val_loss, metric_for_logging, train_time, pred_time \u001b[38;5;241m=\u001b[39m get_val_loss(\n\u001b[0;32m    352\u001b[0m         config_dic,\n\u001b[0;32m    353\u001b[0m         estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m         free_mem_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    367\u001b[0m     )\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     val_loss, metric_for_logging, train_time, pred_time \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model_CV\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbest_val_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv_score_agg_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_training_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_training_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, TransformersEstimator):\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m], fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_val\u001b[39m\u001b[38;5;124m\"\u001b[39m], fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_val\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\task\\generic_task.py:737\u001b[0m, in \u001b[0;36mGenericTask.evaluate_model_CV\u001b[1;34m(self, config, estimator, X_train_all, y_train_all, budget, kf, eval_metric, best_val_loss, cv_score_agg_func, log_training_metric, fit_kwargs, free_mem_ratio)\u001b[0m\n\u001b[0;32m    734\u001b[0m         groups_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    736\u001b[0m estimator\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m--> 737\u001b[0m val_loss_i, metric_i, train_time_i, pred_time_i \u001b[38;5;241m=\u001b[39m \u001b[43mget_val_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbudget_per_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_training_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_training_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric_i, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_results\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m metric_i\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m metric_i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_results\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\ml.py:494\u001b[0m, in \u001b[0;36mget_val_loss\u001b[1;34m(config, estimator, X_train, y_train, X_val, y_val, weight_val, groups_val, eval_metric, task, labels, budget, log_training_metric, fit_kwargs, free_mem_ratio)\u001b[0m\n\u001b[0;32m    489\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# if groups_val is not None:\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m#     fit_kwargs['groups_val'] = groups_val\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m#     fit_kwargs['X_val'] = X_val\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m#     fit_kwargs['y_val'] = y_val\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m val_loss, metric_for_logging, pred_time, _ \u001b[38;5;241m=\u001b[39m _eval_estimator(\n\u001b[0;32m    496\u001b[0m     config,\n\u001b[0;32m    497\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m     fit_kwargs,\n\u001b[0;32m    509\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_results\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\model.py:1653\u001b[0m, in \u001b[0;36mXGBoostSklearnEstimator.fit\u001b[1;34m(self, X_train, y_train, budget, free_mem_ratio, **kwargs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_method\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1652\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_per_trial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_mem_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\model.py:1405\u001b[0m, in \u001b[0;36mLGBMEstimator.fit\u001b[1;34m(self, X_train, y_train, budget, free_mem_ratio, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m   1404\u001b[0m         callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1405\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;66;03m# for xgboost>=1.6.0, pop callbacks to enable pickle\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\automl\\model.py:217\u001b[0m, in \u001b[0;36mBaseEstimator._fit\u001b[1;34m(self, X_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# xgboost 1.6 doesn't display all the params in the model str\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflaml.automl.model - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fit started with params \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[0;32m    219\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflaml.automl.model - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fit finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBClassifier.fit() got an unexpected keyword argument 'search_space'"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators' : (n_estimators_min, n_estimators_max),\n",
    "    'learning_rate' : (learning_rate_min, learning_rate_max),\n",
    "    'max_depth' : (max_depth_min, max_depth_max),\n",
    "    'subsample' : (subsample_min, subsample_max),\n",
    "    'colsample_bytree' : (colsample_bytree_min, colsample_bytree_max),\n",
    "    'gamma' : (gamma_min, gamma_max),\n",
    "    'min_child_weight' : (min_child_weight_min, min_child_weight_max),\n",
    "    'reg_lambda' : (reg_lambda_min, reg_lambda_max),\n",
    "    'reg_alpha' : (reg_alpha_min, reg_alpha_max),\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "\n",
    "config = {\n",
    "    \"max_iter\" :  1024,\n",
    "    \"task\": \"classification\",\n",
    "    \"estimator_list\": ['xgboost'],\n",
    "    #\"n_iterations\": 1024,  \n",
    "    \"metric\": \"f1_weighted\",\n",
    "    \"search_space\": param_space,\n",
    "    \"cv\": cv,\n",
    "}\n",
    "\n",
    "automl = AutoML()\n",
    "\n",
    "automl.fit(x_wine.values, y_wine.values, **config)\n",
    "\n",
    "best_config = automl.best_config\n",
    "print(\"Best Hyperparameters:\", best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import tune\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Define the XGBoost objective function\n",
    "def xgboost_objective(config):\n",
    "    # Extract hyperparameters from the configuration\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',  # Use 'hist' for faster training\n",
    "        'booster': 'gbtree',\n",
    "        'verbosity': 0,\n",
    "        'n_jobs': -1,\n",
    "        **config\n",
    "    }\n",
    "\n",
    "    # Generate dummy training data and labels (replace with your actual data)\n",
    "    np.random.seed(42)\n",
    "    train_data = np.random.rand(100, 10)\n",
    "    train_labels = np.random.randint(0, 2, 100)\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    model = xgb.train(params, xgb.DMatrix(train_data, label=train_labels))\n",
    "\n",
    "    # Generate dummy validation data and labels (replace with your actual data)\n",
    "    validation_data = np.random.rand(20, 10)\n",
    "    validation_labels = np.random.randint(0, 2, 20)\n",
    "\n",
    "    # Evaluate the model on the validation data\n",
    "    predictions = model.predict(xgb.DMatrix(validation_data))\n",
    "\n",
    "    # Define the evaluation metric (logloss for binary classification)\n",
    "    logloss = log_loss(validation_labels, predictions)\n",
    "\n",
    "    # Return the metric to be minimized\n",
    "    return logloss\n",
    "\n",
    "# Step 4: Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'learning_rate': tune.loguniform(lower=1e-5, upper=1),\n",
    "    'max_depth': tune.randint(lower=1, upper=10),\n",
    "    'subsample': tune.uniform(lower=0.1, upper=1.0),\n",
    "    'colsample_bytree': tune.uniform(lower=0.1, upper=1.0),\n",
    "    'n_estimators': tune.randint(lower=50, upper=500),\n",
    "    # Add more hyperparameters as needed\n",
    "}\n",
    "\n",
    "# Step 5: Run FLAML Tune to optimize the XGBoost model\n",
    "flaml_tune_config = {\n",
    "    'time_budget': 600,  # Maximum time for tuning in seconds\n",
    "    'metric': 'minimize',  # Specify 'minimize' for logloss, 'maximize' for AUC, etc.\n",
    "}\n",
    "\n",
    "best_config = tune.run(xgboost_objective, config=search_space, **flaml_tune_config).get_best_config()\n",
    "\n",
    "# Display the best hyperparameter configuration\n",
    "print(\"Best Hyperparameter Configuration:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 01-18 20:20:02] {529} INFO - Using search algorithm BlendSearch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 20:20:02,855 - flaml.tune.searcher.blendsearch - INFO - No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'. More info can be found at https://microsoft.github.io/FLAML/docs/FAQ#about-low_cost_partial_config-in-tune\n",
      "2024-01-18 20:20:02,856 - flaml.tune.searcher.suggestion - WARNING - You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 01-18 20:20:02] {805} INFO - trial 1 config: {'n_estimators': 597, 'learning_rate': 0.04011959752011332, 'max_depth': 4, 'subsample': 0.39466876705417214, 'colsample_bytree': 0.4949275602144958, 'gamma': 7.55486898036596, 'min_child_weight': 2.6639242043080236, 'reg_lambda': 0.756210682605616, 'reg_alpha': 0.013611566647889872}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felps\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\tune\\searcher\\suggestion.py:880: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\n",
      "  return ot.distributions.IntUniformDistribution(\n",
      "C:\\Users\\Felps\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\tune\\searcher\\suggestion.py:865: FutureWarning: LogUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n",
      "  return ot.distributions.LogUniformDistribution(domain.lower, domain.upper)\n",
      "C:\\Users\\Felps\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\flaml\\tune\\searcher\\suggestion.py:870: FutureWarning: UniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.FloatDistribution` instead.\n",
      "  return ot.distributions.UniformDistribution(domain.lower, domain.upper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter Configuration:\n",
      "{'n_estimators': 597, 'learning_rate': 0.04011959752011332, 'max_depth': 4, 'subsample': 0.39466876705417214, 'colsample_bytree': 0.4949275602144958, 'gamma': 7.55486898036596, 'min_child_weight': 2.6639242043080236, 'reg_lambda': 0.756210682605616, 'reg_alpha': 0.013611566647889872}\n"
     ]
    }
   ],
   "source": [
    "from flaml import tune\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Define your data and target variables (replace with your actual data)\n",
    "#x_wine = np.random.rand(100, 10)\n",
    "#y_wine = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Define the parameter search space\n",
    "param_space = {\n",
    "    'n_estimators': tune.randint(lower=n_estimators_min, upper=n_estimators_max),\n",
    "    'learning_rate': tune.loguniform(lower=learning_rate_min, upper=learning_rate_max),\n",
    "    'max_depth': tune.randint(lower=max_depth_min, upper=max_depth_max),\n",
    "    'subsample': tune.uniform(lower=subsample_min, upper=subsample_max),\n",
    "    'colsample_bytree': tune.uniform(lower=colsample_bytree_min, upper=colsample_bytree_max),\n",
    "    'gamma': tune.uniform(lower=gamma_min, upper=gamma_max),\n",
    "    'min_child_weight': tune.uniform(lower=min_child_weight_min, upper=min_child_weight_max),\n",
    "    'reg_lambda': tune.uniform(lower=reg_lambda_min, upper=reg_lambda_max),\n",
    "    'reg_alpha': tune.uniform(lower=reg_alpha_min, upper=reg_alpha_max),\n",
    "}\n",
    "\n",
    "# Define the custom scorer for f1_weighted\n",
    "scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = KFold(n_splits=KFOLD_SPLITS, shuffle=True)\n",
    "\n",
    "# Define the objective function for FLAML Tune\n",
    "def xgboost_objective(config):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'hist',\n",
    "        'booster': 'gbtree',\n",
    "        'verbosity': 0,\n",
    "        **config\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Perform cross-validation with the current configuration\n",
    "    scores = cross_val_score(model, x_wine, y_wine, cv=cv, scoring=scorer)\n",
    "\n",
    "    # Return the average f1_weighted score for minimization\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Set up FLAML Tune configuration\n",
    "flaml_tune_config = {\n",
    "    'mode': 'max',\n",
    "}\n",
    "\n",
    "# Run FLAML Tune\n",
    "best_config = tune.run(xgboost_objective, config=param_space, **flaml_tune_config).get_best_config()\n",
    "\n",
    "# Display the best hyperparameter configuration\n",
    "print(\"Best Hyperparameter Configuration:\")\n",
    "print(best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (210613582.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[51], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    Ajustar flaml,\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#TODO:\n",
    "Ajustar flaml,\n",
    "preparar os outros datasets \n",
    "rodar tudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
