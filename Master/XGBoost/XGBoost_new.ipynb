{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import system\n",
    "from numpy import inf\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import pickle\n",
    "from pymoo.algorithms.soo.nonconvex.pso import PSO, PSOAnimation\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.factory import get_termination\n",
    "from pymoo.core.callback import Callback\n",
    "\n",
    "from pymoo.core.mixed import MixedVariableGA\n",
    "from pymoo.core.variable import Real, Integer\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "from pymoo.util.display.column import Column\n",
    "from pymoo.util.display.output import Output\n",
    "\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "from pymoo.core.variable import Real, Integer, Choice, Binary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../preprocessing/processed_data/wine.csv')\n",
    "y_wine = df[['quality']]\n",
    "x_wine = df.drop(columns='quality')\n",
    "x_wine = x_wine.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 32\n",
    "POPULATION = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFOLD_SPLITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_min = 3\n",
    "n_estimators_max = 1000\n",
    "learning_rate_min = 0.001\n",
    "learning_rate_max = 0.3\n",
    "max_depth_min = 3\n",
    "max_depth_max = 15\n",
    "subsample_min = 0.05\n",
    "subsample_max = 1.0\n",
    "colsample_bytree_min = 0.2\n",
    "colsample_bytree_max = 1.0\n",
    "gamma_min = 0\n",
    "gamma_max = 10\n",
    "min_child_weight_min = 0\n",
    "min_child_weight_max = 10\n",
    "reg_lambda_min = 0\n",
    "reg_lambda_max = 1\n",
    "reg_alpha_min = 0\n",
    "reg_alpha_max = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOutput(Output):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        global pbar \n",
    "        pbar = tqdm(total=ITERATIONS)\n",
    "        self.score = Column(\"score\", width=13)\n",
    "        self.Parameters = Column(\"Parameters\", width=35)\n",
    "        self.columns += [self.score, self.Parameters]\n",
    "\n",
    "    def update(self, algorithm):\n",
    "        super().update(algorithm)\n",
    "        self.score.set(-np.min(algorithm.pop.get(\"F\")))\n",
    "        #self.Parameters.set(algorithm.pop.get(\"X\")[0])\n",
    "        pbar.update(1)\n",
    "        if pbar.n == ITERATIONS: pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedVariableProblem(ElementwiseProblem):\n",
    "    def __init__(self, **kwargs):\n",
    "        #params reference 1\n",
    "        xgb_params = {\n",
    "        'n_estimators' : Integer(bounds=(n_estimators_min, n_estimators_max)), #high num\n",
    "        'learning_rate' : Real(bounds=(learning_rate_min, learning_rate_max)),\n",
    "        'max_depth' : Integer(bounds=(max_depth_min, max_depth_max)),\n",
    "        'subsample' : Real(bounds=(subsample_min, subsample_max)),\n",
    "        'colsample_bytree' : Real(bounds=(colsample_bytree_min, colsample_bytree_max)),\n",
    "        'gamma'            : Real(bounds=(gamma_min, gamma_max)),\n",
    "        'min_child_weight' : Real(bounds=(min_child_weight_min, min_child_weight_max)),\n",
    "        'reg_lambda'       : Real(bounds=(reg_lambda_min, reg_lambda_max)), #l2 lambda\n",
    "        'reg_alpha'        : Real(bounds=(reg_alpha_min, reg_alpha_max)), #L1 alpha\n",
    "        }\n",
    "        super().__init__(vars=xgb_params, n_obj=1, **kwargs)\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        n_estimators = X['n_estimators']\n",
    "        learning_rate = X['learning_rate']\n",
    "        max_depth = X['max_depth']\n",
    "        subsample = X['subsample']\n",
    "        colsample_bytree = X['colsample_bytree']\n",
    "        gamma = X['gamma']\n",
    "        min_child_weight = X['min_child_weight']\n",
    "        reg_lambda = X['reg_lambda']\n",
    "        reg_alpha = X['reg_alpha']\n",
    "        \n",
    "        model_xgboost = xgb.XGBClassifier(\n",
    "            n_estimators = n_estimators,\n",
    "            learning_rate = learning_rate,\n",
    "            max_depth = max_depth,\n",
    "            subsample = subsample,\n",
    "            colsample_bytree = colsample_bytree,\n",
    "            gamma = gamma,\n",
    "            min_child_weight = min_child_weight,\n",
    "            reg_lambda = reg_lambda,\n",
    "            reg_alpha = reg_alpha,\n",
    "            n_jobs = -1\n",
    "            )\n",
    "        \n",
    "        kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "        \n",
    "        scores = cross_val_score(model_xgboost,  x_wine, y_wine, cv = kfold, n_jobs=-1, scoring='f1_weighted')  \n",
    "        result = scores.mean()     \n",
    "        if result == np.nan:\n",
    "            result = 0\n",
    "        #print(result)\n",
    "        #return result\n",
    "        out[\"F\"] = -result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f1_ga(ITERATIONS=32, POPULATION=32):\n",
    "    problem = MixedVariableProblem()\n",
    "\n",
    "    algorithm = MixedVariableGA(pop_size=POPULATION)\n",
    "\n",
    "    term = get_termination(\"n_gen\", ITERATIONS)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    res = minimize(problem,\n",
    "                   algorithm,\n",
    "                   termination=term,\n",
    "                   verbose=True,\n",
    "                   output=MyOutput(),\n",
    "                   # seed=1,\n",
    "                   )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    #print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "    #print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))\n",
    "    params, score = res.X, res.F[0]\n",
    "    return params, -score, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_ga(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_f1_ga()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['GA'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pymoo.org/algorithms/soo/pso.html?highlight=pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_range(x, max_val, min_val):\n",
    "    scaled_value = min_val + (max_val - min_val) * (x / 1000)\n",
    "    \n",
    "    return scaled_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_integers(positions, lower_bound, upper_bound):\n",
    "    return np.clip(np.round(positions), lower_bound, upper_bound).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyswarms as ps\n",
    "from pyswarms.utils.functions import single_obj as fx\n",
    "import numpy as np\n",
    "\n",
    "n_estimators_min_temp, max_depth_min_temp = 0, 0\n",
    "xl=np.array([\n",
    "            n_estimators_min_temp, #n_estimators_min,\n",
    "            learning_rate_min,\n",
    "            max_depth_min_temp, #max_depth_min,\n",
    "            subsample_min,\n",
    "            colsample_bytree_min,\n",
    "            gamma_min,\n",
    "            min_child_weight_min,\n",
    "            reg_lambda_min,\n",
    "            reg_alpha_min,\n",
    "            ])\n",
    "n_estimators_max_temp, max_depth_max_temp = 1, 1\n",
    "xu=np.array([\n",
    "            n_estimators_max_temp, #n_estimators_max,\n",
    "            learning_rate_max,\n",
    "            max_depth_max_temp, #max_depth_max,\n",
    "            subsample_max,\n",
    "            colsample_bytree_max,\n",
    "            gamma_max,\n",
    "            min_child_weight_max,\n",
    "            reg_lambda_max,\n",
    "            reg_alpha_max,\n",
    "            ])\n",
    "\n",
    "def PSO_Optimize_F1(values):\n",
    "    x = values[0] \n",
    "    model_xgboost = xgb.XGBClassifier(\n",
    "        n_estimators = round_to_integers(x[0] * n_estimators_max, n_estimators_min, n_estimators_max),\n",
    "        learning_rate = x[1],\n",
    "        max_depth = round_to_integers(x[2] * max_depth_max, max_depth_min, max_depth_max),\n",
    "        subsample = x[3],\n",
    "        colsample_bytree = x[4],\n",
    "        gamma = x[5],\n",
    "        min_child_weight = x[6],\n",
    "        reg_lambda = x[7],\n",
    "        reg_alpha = x[8],\n",
    "        n_jobs = -1\n",
    "        )\n",
    "    \n",
    "    kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "    \n",
    "    scores = cross_val_score(model_xgboost,  x_wine, y_wine, cv = kfold, n_jobs=-1, scoring='f1_weighted')  \n",
    "    result = scores.mean()     \n",
    "\n",
    "    return -result\n",
    "\n",
    "def run_f1_pso():\n",
    "    # Call an instance of PSO\n",
    "    swarm_size = POPULATION\n",
    "    iters = ITERATIONS\n",
    "    dim = 9\n",
    "    options = {'c1': 1.5, 'c2':1.5, 'w':0.5}\n",
    "    constraints = (xl,xu)\n",
    "\n",
    "    optimizer = ps.single.GlobalBestPSO(n_particles=swarm_size,\n",
    "                                        dimensions=dim,\n",
    "                                        options=options,\n",
    "                                        ftol = -inf,\n",
    "                                        bounds=constraints)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    cost, joint_vars = optimizer.optimize(objective_func = PSO_Optimize_F1, iters=iters)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time    \n",
    "\n",
    "    params = []\n",
    "    for idx in range(len(joint_vars)):\n",
    "        if idx == 0:\n",
    "            params.append(round_to_integers(joint_vars[idx] * n_estimators_max, n_estimators_min, n_estimators_max))\n",
    "        elif idx == 2:\n",
    "            params.append(round_to_integers(joint_vars[idx] * max_depth_max, max_depth_min, max_depth_max))\n",
    "        else:\n",
    "            params.append(joint_vars[idx])\n",
    "\n",
    "    params_dict = {\n",
    "        'n_estimators' : params[0],\n",
    "        'learning_rate' : params[1],\n",
    "        'max_depth' : params[2],\n",
    "        'subsample' : params[3],\n",
    "        'colsample_bytree' : params[4],\n",
    "        'gamma' : params[5],\n",
    "        'min_child_weight' : params[6],\n",
    "        'reg_lambda' : params[7],\n",
    "        'reg_alpha' : params[8],\n",
    "                }\n",
    "    return params_dict, -cost, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_pso(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_f1_pso()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['PSO'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def random_search_int_range(min_l, max_l):\n",
    "    return (np.arange(randint.ppf(0.01, min_l, max_l),randint.ppf(0.99, min_l, max_l))).astype(int)\n",
    "\n",
    "\n",
    "def run_random_search_f1(iterations = 1024):\n",
    "    param_distribution = {\n",
    "    'n_estimators' : round_to_integers(uniform.rvs(0, 1, 1000) * n_estimators_max, n_estimators_min, n_estimators_max),\n",
    "    'learning_rate' : uniform(learning_rate_min, learning_rate_max),\n",
    "    'max_depth' : round_to_integers(uniform.rvs(0, 1, 1000) * max_depth_max, max_depth_min, max_depth_max),\n",
    "    'subsample' : uniform(subsample_min, subsample_max),\n",
    "    'colsample_bytree' : uniform(colsample_bytree_min, colsample_bytree_max),\n",
    "    'gamma' : uniform(gamma_min, gamma_max),\n",
    "    'min_child_weight' : random_search_int_range(min_child_weight_min, min_child_weight_max),\n",
    "    'reg_alpha' : uniform(reg_alpha_min, reg_alpha_max),\n",
    "    'reg_lambda' : uniform(reg_lambda_min, reg_lambda_max),\n",
    "    #Not used\n",
    "    #'scale_pos_weight' : random_search_int_range(scale_pos_weight_min, scale_pos_weight_max),\n",
    "    #'base_score' : uniform(base_score_min, base_score_max),\n",
    "    'n_jobs' :  [-1]\n",
    "    }\n",
    "    \n",
    "    kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    random_search_f1 = RandomizedSearchCV(xgb.XGBClassifier(), param_distribution, n_iter = iterations, n_jobs = -1, cv=kfold, verbose=False, scoring='f1_weighted')\n",
    "    random_search_f1.fit(x_wine, y_wine)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    params = random_search_f1.best_params_\n",
    "    score = random_search_f1.best_score_\n",
    "    \n",
    "    \n",
    "    return params, score, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_rs(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_random_search_f1()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['RS'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective_f1(trial):\n",
    "  \n",
    "      n_estimators = trial.suggest_int('n_estimators', n_estimators_min, n_estimators_max)\n",
    "      learning_rate = trial.suggest_float('learning_rate', learning_rate_min, learning_rate_max)\n",
    "      max_depth = trial.suggest_int('max_depth', max_depth_min, max_depth_max)\n",
    "      subsample = trial.suggest_float('subsample', subsample_min, subsample_max)\n",
    "      colsample_bytree = trial.suggest_float('colsample_bytree', colsample_bytree_min, colsample_bytree_max)\n",
    "      gamma = trial.suggest_float('gamma', gamma_min, gamma_max)\n",
    "      min_child_weight = trial.suggest_float('min_child_weight', min_child_weight_min, min_child_weight_max)\n",
    "      reg_alpha = trial.suggest_float('reg_alpha', reg_alpha_min, reg_alpha_max)\n",
    "      reg_lambda = trial.suggest_float('reg_lambda', reg_lambda_min, reg_lambda_max)\n",
    "      \n",
    "      model_xgboost = xgb.XGBClassifier(\n",
    "            n_estimators = n_estimators,\n",
    "            learning_rate = learning_rate,\n",
    "            max_depth = max_depth,\n",
    "            subsample = subsample,\n",
    "            colsample_bytree = colsample_bytree,\n",
    "            gamma = gamma,\n",
    "            min_child_weight = min_child_weight,\n",
    "            reg_lambda = reg_lambda,\n",
    "            reg_alpha = reg_alpha,\n",
    "            n_jobs = -1\n",
    "            )\n",
    "\n",
    "      \n",
    "      kfold = KFold(n_splits = KFOLD_SPLITS, shuffle = True)\n",
    "      \n",
    "      result = cross_val_score(model_xgboost, x_wine, y_wine, n_jobs=-1, scoring='f1_weighted', cv=kfold).mean()\n",
    "      if result == np.nan:\n",
    "            result = 0\n",
    "\n",
    "      return result\n",
    "\n",
    "def run_optuna_f1(n_trials=1024):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    study.optimize(objective_f1, n_trials=n_trials, n_jobs = -1)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time    \n",
    "    trial = study.best_trial\n",
    "    #print('F1: {}'.format(trial.value))\n",
    "    #print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "    return trial.params, trial.value, execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run_optuna(iterations):\n",
    "    filename = './XGBoost_wine_data.csv'\n",
    "    try:\n",
    "        XGBoost_data = pd.read_csv(filename)\n",
    "    except:\n",
    "        open(filename, \"a\")\n",
    "        XGBoost_data = pd.DataFrame(columns=['Algorithm', 'Model', 'F1', 'Time', 'params'])\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(iterations)):\n",
    "        params, score, execution_time = run_optuna_f1()\n",
    "        \n",
    "        temp = pd.DataFrame({'Algorithm' : ['Optuna'], \n",
    "                             'Model' : ['XGBoost'], \n",
    "                            'F1' : [score], \n",
    "                            'Time' : [execution_time],\n",
    "                            'params' : [params]\n",
    "                            })\n",
    "        \n",
    "        XGBoost_data = pd.concat([XGBoost_data, temp], ignore_index=True)\n",
    "        XGBoost_data[['Algorithm', 'Model', 'F1', 'Time', 'params']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import time\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "def objective_f1(config, x_train, y_train, cv):\n",
    "    n_estimators = int(config[\"n_estimators\"])\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    max_depth = int(config[\"max_depth\"])\n",
    "    subsample = config[\"subsample\"]\n",
    "    colsample_bytree = config[\"colsample_bytree\"]\n",
    "    gamma = config[\"gamma\"]\n",
    "    min_child_weight = config[\"min_child_weight\"]\n",
    "    reg_alpha = config[\"reg_alpha\"]\n",
    "    reg_lambda = config[\"reg_lambda\"]\n",
    "\n",
    "    model_xgboost = xgb.XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        gamma=gamma,\n",
    "        min_child_weight=min_child_weight,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    kfold = KFold(n_splits=cv, shuffle=True)\n",
    "    \n",
    "    result = cross_val_score(model_xgboost, x_train, y_train, n_jobs=-1, scoring='f1_weighted', cv=kfold).mean()\n",
    "    if np.isnan(result):\n",
    "        result = 0\n",
    "\n",
    "    return result\n",
    "\n",
    "def run_flaml_f1(x_train, y_train, n_trials=1024, cv=3):\n",
    "    automl = AutoML()\n",
    "    \n",
    "    settings = {\n",
    "        \"time_budget\": 600,  # set your desired time budget\n",
    "        \"metric\": 'f1',\n",
    "        \"task\": 'classification',\n",
    "        \"log_file_name\": 'flaml.log',\n",
    "    }\n",
    "    \n",
    "    automl.fit(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        task='classification',\n",
    "        estimator_list=['xgboost'],\n",
    "        eval_method='f1',\n",
    "        cv=cv,\n",
    "        **settings\n",
    "    )\n",
    "\n",
    "    best_trial = automl.best_config\n",
    "    best_value = automl.best_perf\n",
    "    execution_time = automl.time_elapsed\n",
    "\n",
    "    return best_trial, best_value, execution_time\n",
    "\n",
    "# Example usage\n",
    "# best_trial, best_value, execution_time = run_flaml_f1(x_train, y_train, n_trials=1024, cv=5)\n",
    "# print(\"Best hyperparameters:\", best_trial)\n",
    "# print(\"Best f1 score:\", best_value)\n",
    "# print(\"Execution time:\", execution_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost classifier class\n",
    "class XGBoostClassifier(xgb.XGBClassifier):\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return super().fit(X, y, **kwargs)\n",
    "\n",
    "# Define the search space for hyperparameter optimization\n",
    "param_space = {\n",
    "    \"n_estimators\": (10, 1000),\n",
    "    \"learning_rate\": (0.01, 1.0),\n",
    "    \"max_depth\": (3, 10),\n",
    "    \"subsample\": (0.1, 1.0),\n",
    "    \"colsample_bylevel\": (0.1, 1.0),\n",
    "}\n",
    "\n",
    "# Set up cross-validation (Stratified K-Fold in this example)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the FLAML configuration\n",
    "config = {\n",
    "    \"task\": \"classification\",\n",
    "    \"estimator\": XGBoostClassifier,\n",
    "    \"time_budget\": 60,\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"hyperparam_space\": param_space,\n",
    "    \"cv\": cv,  # Specify the cross-validation strategy\n",
    "}\n",
    "\n",
    "# Create an AutoML instance\n",
    "automl = AutoML()\n",
    "\n",
    "# Run FLAML to optimize the XGBoost classifier with hyperparameter optimization and cross-validation\n",
    "automl.fit(X_train, y_train, **config)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = automl.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the best hyperparameters found by FLAML\n",
    "best_config = automl.best_config\n",
    "print(\"Best Hyperparameters:\", best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
